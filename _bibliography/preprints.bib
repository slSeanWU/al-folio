@article{wu2021musemorphose,
  abbr={ArXiv},
  field={Music},
  field_badge_class={badge_music},
  abstract={Transformers and variational autoencoders (VAE) have been extensively employed for symbolic (e.g., MIDI) domain music generation. While the former boast an impressive capability in modeling long sequences, the latter allow users to willingly exert control over different parts (e.g., bars) of the music to be generated. In this paper, we are interested in bringing the two together to construct a single model that exhibits both strengths. The task is split into two steps. First, we equip Transformer decoders with the ability to accept segment-level, time-varying conditions during sequence generation. Subsequently, we combine the developed and tested in-attention decoder with a Transformer encoder, and train the resulting MuseMorphose model with the VAE objective to achieve style transfer of long musical pieces, in which users can specify musical attributes including rhythmic intensity and polyphony (i.e., harmonic fullness) they desire, down to the bar level. Experiments show that MuseMorphose outperforms recurrent neural network (RNN) based baselines on numerous widely-used metrics for style transfer tasks.},
  title={MuseMorphose: Full-song and fine-grained music style transfer with one Transformer VAE},
  author={Wu, Shih-Lun and Yang, Yi-Hsuan},
  journal={ArXiv preprint, under review at IEEE/ACM Trans. Audio, Speech, and Language Processing (TASLP)},
  year={2021},
  anticip_year={2022},
  pdf={https://arxiv.org/pdf/2105.04090.pdf},
  website={https://slseanwu.github.io/site-musemorphose/},
  code={https://github.com/YatingMusic/MuseMorphose}
}

@article{wu2022composeembellish,
  abbr={ArXiv},
  field={Music},
  field_badge_class={badge_music},
  abstract={Even with strong sequence models like Transformers, generating expressive piano performances with long-range musical structures remains challenging. Meanwhile, methods to compose well-structured melodies or lead sheets (melody + chords), i.e., simpler forms of music, gained more success. Observing the above, we devise a two-stage Transformer-based framework that Composes a lead sheet first, and then Embellishes it with accompaniment and expressive touches. Such a factorization also enables pretraining on non-piano data. Our objective and subjective experiments show that Compose & Embellish shrinks the gap in structureness between a current state of the art and real performances by half, and improves other musical aspects such as richness and coherence as well.},
  title={Compose & Embellish: Well-Structured Piano Performance Generation via A Two-Stage Approach},
  author={Wu, Shih-Lun and Yang, Yi-Hsuan},
  journal={ArXiv preprint, to be submitted to Int. Conf. on Acoustics, Speech, & Signal Processing (ICASSP)},
  year={2022},
  anticip_year={2023},
  pdf={https://arxiv.org/pdf/2209.08212.pdf}
}